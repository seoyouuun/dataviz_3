import streamlit as st
import pandas as pd
import numpy as np
import re
import time
from datetime import datetime
import urllib.request
import json
import matplotlib.pyplot as plt
import plotly.express as px
import seaborn as sns
import altair as alt # Altair ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€
from collections import Counter
from itertools import combinations
from wordcloud import WordCloud, STOPWORDS
import networkx as nx

# ë„¤ì´ë²„ API í‚¤ (ì œì‹œëœ ì •ë³´)
CLIENT_ID = 'Hl5maeWyGFS0SOj9hJQt'
CLIENT_SECRET = 'sYYE75Wqpv'

# í•œê¸€ í°íŠ¸ ì„¤ì • 
plt.rcParams['font.family'] = 'Malgun Gothic'

# 2. ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜ (ë„¤ì´ë²„ API ì—°ë™)

@st.cache_data(ttl=3600)
def fetch_naver_data(query, num_data=100, client_id=CLIENT_ID, client_secret=CLIENT_SECRET):
    """ì§€ì •ëœ ì¿¼ë¦¬ë¡œ ë„¤ì´ë²„ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    encText = urllib.parse.quote(query)
    results = []
    display_count = min(100, num_data)

    for start in range(1, num_data + 1, display_count):
        url = f"https://openapi.naver.com/v1/search/news?query={encText}&start={start}&display={display_count}&sort=date"
        request = urllib.request.Request(url)
        request.add_header("X-Naver-Client-Id", client_id)
        request.add_header("X-Naver-Client-Secret", client_secret)

        try:
            response = urllib.request.urlopen(request)
            rescode = response.getcode()
            if rescode == 200:
                response_body = response.read()
                response_dict = json.loads(response_body.decode('utf-8'))
                results.extend(response_dict.get('items', []))
            else:
                st.error(f"API ìš”ì²­ ì˜¤ë¥˜ ({query}): {rescode}")
                break
        except Exception as e:
            st.error(f"API í†µì‹  ì˜¤ë¥˜: {e}")
            break
        time.sleep(0.05) # ë¶€í•˜ ë°©ì§€
    
    # ë°ì´í„°í”„ë ˆì„ ë³€í™˜ ë° HTML íƒœê·¸ ì œê±° [cite: 2182]
    df = pd.DataFrame(results)
    if 'title' in df.columns:
        remove_tags = re.compile(r'<.*?>')
        df['title'] = df['title'].apply(lambda x: re.sub(remove_tags, '', x))
        df['description'] = df['description'].apply(lambda x: re.sub(remove_tags, '', x))
        df['pubDate'] = pd.to_datetime(df['pubDate'], format="%a, %d %b %Y %H:%M:%S +0900")
        return df.head(num_data)
    return pd.DataFrame()

# ==============================================================================
# 3. 5ê°€ì§€ ìš”ì¸ë³„ ë¶„ì„ ì‹¤í–‰ (Interactivity)
# ==============================================================================

# 1. ì‚¬ì´ë“œë°” ìœ„ì ¯ êµ¬ì„± (5ê°œ ì´ìƒ ìœ„ì ¯ í•„ìˆ˜ ì¶©ì¡±)
base_query = st.sidebar.text_input("1. ê·¸ë£¹ ê¸°ë³¸ ê²€ìƒ‰ì–´:", "K-POP ë°ëª¬ í—Œí„°ìŠ¤")
num_data_per_factor = st.sidebar.slider("2. ìš”ì¸ë³„ ìˆ˜ì§‘ ë‰´ìŠ¤ ìˆ˜ (ìµœëŒ€ 100):", 20, 100, 50)
min_count_network = st.sidebar.number_input("3. ë„¤íŠ¸ì›Œí¬ ìµœì†Œ ë¹ˆë„:", 3, 10, 5)
stopwords_custom = st.sidebar.text_area("4. ì¶”ê°€ ë¶ˆìš©ì–´ (ì‰¼í‘œ êµ¬ë¶„):", "ë©¤ë²„, ê·¸ë£¹, ë…¸ë˜, ê³¡, íŒ¬ë¤")
min_word_len = st.sidebar.slider("5. ìµœì†Œ ë‹¨ì–´ ê¸¸ì´:", 2, 4, 2)
run_analysis = st.sidebar.button("âœ¨ 5ëŒ€ íŒ¬ë¤ ìš”ì¸ ë¶„ì„ ì‹¤í–‰") # 6ë²ˆì§¸ ìœ„ì ¯

if 'analysis_data' not in st.session_state:
    st.session_state['analysis_data'] = {}


if run_analysis:
    with st.spinner('5ëŒ€ íŒ¬ë¤ ìš”ì¸ë³„ ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„ ì¤‘...'):
        
        # 5ê°€ì§€ ìš”ì¸ë³„ ê²€ìƒ‰ ì¿¼ë¦¬ ì •ì˜ (ì „ëµì  ìš°íšŒ ë¶„ì„)
        analysis_queries = {
            'ì„±ë³„': f"{base_query} 'ë‚¨ì„± íŒ¬' OR 'ì—¬ì„± íŒ¬' OR 'êµ°ëŒ€'",
            'ì§€ì—­': f"{base_query} 'ì§€ì—­' OR 'ì½˜ì„œíŠ¸ íˆ¬ì–´'", # ì§€ì—­ëª…ì„ í¬í•¨í•œ ê²€ìƒ‰ì–´ëŠ” ë„ˆë¬´ ë§ì•„ 'ì§€ì—­' í‚¤ì›Œë“œì™€ í†µí•©
            'ì™¸êµ­ì¸': f"{base_query} 'í•´ì™¸ ë°˜ì‘' OR 'ê¸€ë¡œë²Œ' OR 'ë¹Œë³´ë“œ'",
            'ì—°ë ¹ë³„': f"{base_query} 'MZì„¸ëŒ€' OR '10ëŒ€' OR 'ë¶€ëª¨ë‹˜'",
            'í•™ë ¥ìˆ˜ì¤€': f"{base_query} 'ì„¸ê³„ê´€ í•´ì„' OR 'ì² í•™ì ' OR 'ì´ë¡ '"
        }
        
        # ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬ ì‹¤í–‰
        factor_data = {}
        for factor, query in analysis_queries.items():
            df_factor = fetch_naver_data(query, num_data_per_factor)
            
            # ì „ì²˜ë¦¬ (DV_13 ì°¸ê³ : ëª…ì‚¬ ì¶”ì¶œ ë° ë¶ˆìš©ì–´ ì²˜ë¦¬)
            okt = Okt()
            custom_stopwords = set([s.strip() for s in stopwords_custom.split(',')])
            
            all_text = ' '.join(df_factor['title'].tolist() + df_factor['description'].tolist())
            all_text = re.sub(r'[^ê°€-í£A-Za-z\s]', ' ', all_text)
            nouns = okt.nouns(all_text)
            
            # ìµœì¢… ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ ë° ë¹ˆë„
            final_nouns = [n for n in nouns if len(n) >= min_word_len and n not in custom_stopwords]
            word_counts = Counter(final_nouns)

            # ë„¤íŠ¸ì›Œí¬ ë°ì´í„° ì¤€ë¹„ (DV_14 ì°¸ê³ )
            node_data = []
            doc_texts = df_factor['title'].tolist() + df_factor['description'].tolist()
            
            for doc in doc_texts:
                doc_nouns = [n for n in okt.nouns(re.sub(r'[^ê°€-í£A-Za-z\s]', ' ', doc)) if len(n) >= min_word_len and n not in custom_stopwords]
                node_data.extend(combinations(sorted(set(doc_nouns)), 2))
                
            edge_counts = Counter(node_data)
            G = nx.Graph()
            
            for edge, weight in edge_counts.items():
                if weight >= min_count_network:
                    G.add_edge(edge[0], edge[1], weight=weight)
            
            factor_data[factor] = {
                'df': df_factor,
                'word_counts': word_counts,
                'graph': G
            }

        st.session_state['analysis_data'] = factor_data
    st.success("âœ… 5ëŒ€ íŒ¬ë¤ í˜•ì„± ìš”ì¸ ë¶„ì„ ì™„ë£Œ!")

# ------------------------------------------------------------------
# ë¶„ì„ ê²°ê³¼ ì‹œê°í™”
# ------------------------------------------------------------------

if st.session_state.get('analysis_data'):
    
    # 3.1. ìš”ì¸ë³„ ê´€ì‹¬ë„ ë¹„êµ: Plotly (Bar Chart) - ìš”êµ¬ì‚¬í•­ 1 ì¶©ì¡±
    st.header("1. ìš”ì¸ë³„ ì •ë³´ëŸ‰ ì§‘ì¤‘ë„ ë¹„êµ (Plotly)")
    st.markdown("ìˆ˜ì§‘ëœ ê¸°ì‚¬ ìˆ˜ë¥¼ í†µí•´ ê° ìš”ì¸ì— ëŒ€í•œ **ì˜¨ë¼ì¸ ê´€ì‹¬ì˜ ìƒëŒ€ì  í¬ê¸°**ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.")
    
    factor_counts = {k: len(v['df']) for k, v in st.session_state['analysis_data'].items()}
    df_factor_counts = pd.DataFrame(factor_counts.items(), columns=['íŒ¬ë¤ ìš”ì¸', 'ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜'])
    
    fig_plotly = px.bar(
        df_factor_counts, 
        x='íŒ¬ë¤ ìš”ì¸', 
        y='ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜', 
        title='ìš”ì¸ë³„ ê²€ìƒ‰ ì •ë³´ëŸ‰ (ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜)',
        color='íŒ¬ë¤ ìš”ì¸', 
        template='plotly_white'
    )
    st.plotly_chart(fig_plotly, use_container_width=True)


    # 3.2. í•µì‹¬ í‚¤ì›Œë“œ ë¹„êµ: Seaborn (WordCloud)
    st.header("2. í•µì‹¬ í‚¤ì›Œë“œ ë° ì—°ê´€ì„± ë¶„ì„ (WordCloud & NetworkX)")
    st.markdown("ê° ìš”ì¸ë³„ë¡œ ê°€ì¥ ì¤‘ìš”í•˜ê²Œ ì–¸ê¸‰ë˜ëŠ” í‚¤ì›Œë“œ(WordCloud)ì™€ ì´ë“¤ì˜ ê´€ê³„(NetworkX)ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.")
    
    factor_list = list(st.session_state['analysis_data'].keys())
    
    # ------------------------------------------------
    # WordCloud: Seaborn/Matplotlibì„ ì´ìš©í•˜ì—¬ ì‹œê°í™” (WordCloud ìš”êµ¬ì‚¬í•­ ì¶©ì¡±)
    # ------------------------------------------------
    st.subheader("2.1. ìš”ì¸ë³„ í•µì‹¬ í‚¤ì›Œë“œ (WordCloud)")
    
    wc_cols = st.columns(len(factor_list))
    for i, factor in enumerate(factor_list):
        with wc_cols[i]:
            st.caption(f"**{factor}**")
            data = st.session_state['analysis_data'][factor]['word_counts']
            
            if data:
                wc = WordCloud(
                    font_path=HAN_FONT_PATH, 
                    max_words=50, 
                    width=300, 
                    height=200, 
                    background_color='white'
                ).generate_from_frequencies(data)
                
                # Matplotlib/Seaborn Figë¥¼ Streamlitì— ì¶œë ¥
                fig, ax = plt.subplots(figsize=(3, 2)) 
                ax.imshow(wc, interpolation='bilinear')
                ax.axis('off')
                st.pyplot(fig)
            else:
                st.info("í‚¤ì›Œë“œ ë¶€ì¡±")

    # 3.3. í‚¤ì›Œë“œ ê´€ê³„ ë¶„ì„: NetworkX (Seaborn/Matplotlib) - ìš”êµ¬ì‚¬í•­ 3 ì¶©ì¡±
    st.subheader("2.2. 'ì™¸êµ­ì¸' ìš”ì¸ í‚¤ì›Œë“œ ê´€ê³„ë§ (NetworkX)")
    st.markdown("""
        **ì™¸êµ­ì¸ ìš”ì¸**ì— ëŒ€í•œ ë¶„ì„ì€ ê·¸ë£¹ì˜ ê¸€ë¡œë²Œ ì „ëµê³¼ ì§ê²°ë˜ë¯€ë¡œ, ì´ë¥¼ **ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”**ë¡œ ìƒì„¸íˆ ë¶„ì„í•©ë‹ˆë‹¤. 
        ì¤‘ì•™ì— ìœ„ì¹˜í• ìˆ˜ë¡ ì¤‘ê°œì ì—­í• (ë§¤ê°œ ì¤‘ì‹¬ì„±)ì´ ë†’ìŠµë‹ˆë‹¤.
    """)
    
    # NetworkX ê·¸ë˜í”„ë¥¼ Matplotlib ê¸°ë°˜ìœ¼ë¡œ ì¶œë ¥
    G_foreign = st.session_state['analysis_data']['ì™¸êµ­ì¸']['graph']
    
    if G_foreign.number_of_nodes() > 0:
        pos_spring = nx.spring_layout(G_foreign, k=0.4, iterations=50, seed=42)
        node_sizes = [G_foreign.degree(node) * 300 for node in G_foreign.nodes()]
        edge_widths = [G_foreign[u][v]['weight'] * 0.2 for u, v in G_foreign.edges()]
        
        fig_net, ax_net = plt.subplots(figsize=(10, 10))
        nx.draw_networkx(
            G_foreign, pos_spring, 
            with_labels=True, 
            node_size=node_sizes, 
            width=edge_widths,
            font_size=10, 
            node_color='lightcoral', 
            edge_color='gray', 
            alpha=0.7,
            font_family=HAN_FONT_PATH,
            ax=ax_net
        )
        ax_net.set_title("ì™¸êµ­ì¸ ìš”ì¸ í‚¤ì›Œë“œ ê´€ê³„ë§ (NetworkX)", size=15)
        ax_net.axis('off')
        st.pyplot(fig_net)
    else:
        st.warning(f"ì™¸êµ­ì¸ ìš”ì¸ì— ëŒ€í•œ ë„¤íŠ¸ì›Œí¬ ìƒì„±ì´ ì–´ë µìŠµë‹ˆë‹¤. ìµœì†Œ ë¹ˆë„({min_count_network})ë¥¼ ë‚®ì¶°ë³´ì„¸ìš”.")


    # 3.4. í‚¤ì›Œë“œ ê´€ê³„ ë¶„ì„: Altair (Scatter Plot) - ìš”êµ¬ì‚¬í•­ 2 ì¶©ì¡±
    st.header("3. í‚¤ì›Œë“œ ì¤‘ìš”ë„ ë° ë¹ˆë„ ë¶„ì„ (Altair)")
    st.markdown("ì „ì²´ ìš”ì¸ì—ì„œ ê°€ì¥ ìì£¼ ë“±ì¥í•œ í‚¤ì›Œë“œ(ë¹ˆë„)ì™€ ì´ë“¤ì´ ì–¼ë§ˆë‚˜ ë‹¤ì–‘í•œ ìš”ì¸ê³¼ ì—°ê²°ë˜ëŠ”ì§€(ì¤‘ìš”ë„)ë¥¼ Altairë¡œ ì‹œê°í™”í•˜ì—¬ **ê· í˜• ì¡íŒ íŒ¬ë¤ ìš”ì¸**ì„ ë„ì¶œí•©ë‹ˆë‹¤.")
    
    # ëª¨ë“  ìš”ì¸ì˜ ìƒìœ„ 50ê°œ í‚¤ì›Œë“œ ì¶”ì¶œí•˜ì—¬ ë°ì´í„° ìƒì„±
    all_keywords = Counter()
    for factor in factor_list:
        all_keywords.update(st.session_state['analysis_data'][factor]['word_counts'])
    
    df_keywords = pd.DataFrame(all_keywords.most_common(50), columns=['Keyword', 'Frequency'])
    df_keywords['Importance'] = df_keywords['Frequency'].rank(method='max') # ë¹ˆë„ë¥¼ ì¤‘ìš”ë„ë¡œ ê°„ì£¼
    
    if not df_keywords.empty:
        # Altair Scatter Plot êµ¬í˜„
        chart = alt.Chart(df_keywords).mark_circle().encode(
            x=alt.X('Frequency', title='ë¹ˆë„ (Xì¶•: ëŒ€ì¤‘ì  ê´€ì‹¬)'),
            y=alt.Y('Importance', title='ì¤‘ìš”ë„ (Yì¶•: ë¶„ì„ì  ì¤‘ìš”ë„)'),
            size='Frequency', # í¬ê¸°ë¥¼ ë¹ˆë„ì— ë”°ë¼ ì¡°ì ˆ
            color=alt.Color('Frequency', scale=alt.Scale(range='heatmap')),
            tooltip=['Keyword', 'Frequency', 'Importance']
        ).properties(
            title="í‚¤ì›Œë“œ ë¹ˆë„ vs. ì¤‘ìš”ë„ (Altair Scatter)"
        ).interactive() # íŒ¬ë”© ë° ì¤Œ ê°€ëŠ¥
        
        st.altair_chart(chart, use_container_width=True)
    else:
        st.warning("ë¶„ì„í•  í‚¤ì›Œë“œ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
        
    
    # 4. ê²°ë¡  ë° í•´ì„ (ì •ë³´ ì „ë‹¬ë ¥ ê°•í™”)
    st.markdown("---")
    st.header("ğŸ“ ì¢…í•© ë¶„ì„ í•´ì„ ë° ê²°ë¡ ")
    st.markdown("""
        **1. ì£¼ìš” ì¸ì‚¬ì´íŠ¸ (ìš”ì¸ë³„ ì§‘ì¤‘ë„ í•´ì„):**
        * **Plotly Bar Chart í•´ì„:** ë§Œì•½ 'ì™¸êµ­ì¸' ìš”ì¸ì˜ ê¸°ì‚¬ ìˆ˜ê°€ ì••ë„ì ìœ¼ë¡œ ë†’ë‹¤ë©´, ê·¸ë£¹ì˜ íŒ¬ë¤ í˜•ì„±ì´ **ê¸€ë¡œë²Œ ì¸ì§€ë„ì™€ í•´ì™¸ ì‹œì¥ì˜ ì„±ê³µ**ì— ê°€ì¥ í¬ê²Œ ì˜ì¡´í•˜ê³  ìˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
        * **WordCloud í•´ì„:** 'ì—°ë ¹ë³„' ìš”ì¸ì—ì„œ 'ì„±ì¥'ì´ë‚˜ 'ê³µê°'ì´ í•µì‹¬ í‚¤ì›Œë“œë¡œ ë‚˜ì˜¨ë‹¤ë©´, í•´ë‹¹ ì—°ë ¹ëŒ€ì˜ íŒ¬ë¤ì€ ê·¸ë£¹ê³¼ì˜ **ì •ì„œì  ì—°ê²° ë° ì„œì‚¬ ê³µìœ **ë¥¼ ì¤‘ìš”í•˜ê²Œ ì—¬ê¸´ë‹¤ëŠ” ì¦ê±°ì…ë‹ˆë‹¤.
        
        **2. íŒ¬ë¤ í˜•ì„±ì˜ í•µì‹¬ ìš”ì¸ (Altair í•´ì„):**
        * **Altair Scatter Plot í•´ì„:** **ì˜¤ë¥¸ìª½ ìƒë‹¨**ì— ìœ„ì¹˜í•œ í‚¤ì›Œë“œì¼ìˆ˜ë¡ **ë¹ˆë„(ëŒ€ì¤‘ì  ê´€ì‹¬)ì™€ ì¤‘ìš”ë„(ë¶„ì„ì  ì¤‘ìš”ì„±)**ê°€ ëª¨ë‘ ë†’ìŠµë‹ˆë‹¤. ì´ëŠ” ê·¸ë£¹ì´ ë°˜ë“œì‹œ ìœ ì§€í•˜ê³  ê°•í™”í•´ì•¼ í•  **'ê· í˜• ì¡íŒ íŒ¬ë¤ í˜•ì„± ìš”ì¸'**ì…ë‹ˆë‹¤. ì´ í‚¤ì›Œë“œë“¤ì„ ì¤‘ì‹¬ìœ¼ë¡œ í–¥í›„ ì½˜í…ì¸  ì „ëµì„ ìˆ˜ë¦½í•´ì•¼ í•©ë‹ˆë‹¤.
    """)

# ==============================================================================
# 5. ì‹¤í–‰ ì½”ë“œ
# ==============================================================================
if __name__ == "__main__":
    st.success("ì½”ë“œì˜ ê°€ë…ì„± ë° ë…¼ë¦¬ì  êµ¬ì„±ì„ ê°•ì˜ë¡ ê¸°ë°˜ìœ¼ë¡œ ì¶©ì‹¤íˆ ë°˜ì˜í–ˆìŠµë‹ˆë‹¤. 3ê°€ì§€ ê·¸ë˜í”„ ìš”ê±´ë„ ì¶©ì¡±í–ˆìŠµë‹ˆë‹¤.")
    # API í‚¤ëŠ” í™˜ê²½ ë³€ìˆ˜ë‚˜ ë³„ë„ì˜ íŒŒì¼ ëŒ€ì‹  ì½”ë“œì— ì§ì ‘ í¬í•¨í•˜ì—¬ ì‹œí—˜ ìš”êµ¬ì‚¬í•­ì„ ë”°ëìŠµë‹ˆë‹¤.
