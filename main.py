#%%
import streamlit as st
import pandas as pd
import numpy as np
import re
import time
from datetime import datetime
from konlpy.tag import Okt
from collections import Counter
from itertools import combinations
from wordcloud import WordCloud, STOPWORDS
import networkx as nx
import matplotlib.pyplot as plt
import plotly.express as px
import seaborn as sns # Plotly, NetworkX, Pandas Chart
# %%
# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'Malgun Gothic'
# %%
# í˜ì´ì§€ ì„¤ì • 
st.set_page_config(
    page_title="K-POP ë°ëª¬ í—Œí„°ìŠ¤ ëŒ€ì‹œë³´ë“œ",
    page_icon="ğŸ¤",
    layout="wide",
    initial_sidebar_state="expanded"
)
# %%
# í•™ë²ˆ/ì´ë¦„ í‘œê¸° 
st.sidebar.title("ì œì¶œì ì •ë³´")
st.sidebar.markdown("í•™ë²ˆ: c321081")
st.sidebar.markdown("ì´ë¦„: ê¹€ì„œì—°")
st.sidebar.divider()
# %%
# ë°ì´í„° ìˆ˜ì§‘
@st.cache_data(ttl=3600) # ë°ì´í„° ìºì‹± ì ìš© (DV_12 ì°¸ì¡°: 1ì‹œê°„ ìœ íš¨)
def get_naver_news_data(query, num_data, client_id, client_secret):
    import urllib.request
    import json
    
    # ì¿¼ë¦¬ ì¸ì½”ë”©
    encText = urllib.parse.quote(query)
    
    # API ì„¤ì •
    display_count = 100
    sort = 'date'
    results = []

    st.info(f"'{query}'ì— ëŒ€í•œ ë„¤ì´ë²„ ë‰´ìŠ¤ ë°ì´í„° {num_data}ê±´ ìˆ˜ì§‘ ì¤‘...")

    # í˜ì´ì§€ë³„ë¡œ ìš”ì²­ ë° ë°ì´í„° ìˆ˜ì§‘
    for idx in range(1, num_data + 1, display_count):
        url = f"https://openapi.naver.com/v1/search/news?query={encText}&start={idx}&display={display_count}&sort={sort}"
        request = urllib.request.Request(url)
        request.add_header("X-Naver-Client-Id", client_id)
        request.add_header("X-Naver-Client-Secret", client_secret)

        try:
            response = urllib.request.urlopen(request)
            rescode = response.getcode()
            if rescode == 200:
                response_body = response.read()
                response_dict = json.loads(response_body.decode('utf-8'))
                results.extend(response_dict.get('items', []))
            else:
                st.error(f"Error Code: {rescode}")
                break
        except Exception as e:
            st.error(f"API ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            break
        
        # API ì‚¬ìš©ëŸ‰ ì œí•œì„ ìœ„í•´ ì ì‹œ ëŒ€ê¸°
        time.sleep(0.1)

        # ë°ì´í„°í”„ë ˆì„ ë³€í™˜ ë° ì •ì œ
    df = pd.DataFrame(results)
    if 'title' in df.columns:
        # HTML íƒœê·¸ ì œê±° (DV_11 ì°¸ì¡°: re.sub(remove_tags, "", text))
        remove_tags = re.compile(r'<.*?>')
        df['title'] = df['title'].apply(lambda x: re.sub(remove_tags, '', x))
        df['description'] = df['description'].apply(lambda x: re.sub(remove_tags, '', x))
        
        # ë‚ ì§œ í˜•ì‹ ë³€í™˜
        df['pubDate'] = df['pubDate'].apply(
            lambda x: datetime.strptime(x, "%a, %d %b %Y %H:%M:%S +0900")
        )
        return df.head(num_data)
    else:
        st.error("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì¿¼ë¦¬ ë˜ëŠ” API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return pd.DataFrame()
    
# %%
#í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ë„¤íŠ¸ì›Œí¬ ë¶„ì„ í•¨ìˆ˜
@st.cache_data(ttl=3600)
def preprocess_and_analyze(df, min_count, min_len, stopwords_add):
    if df.empty:
        return [], {}, nx.Graph()

    okt = Okt()
    
    # ë¶ˆìš©ì–´ ì •ì˜ ë° ì¶”ê°€
    # ê°•ì˜ë¡ì˜ ê¸°ë³¸ ë¶ˆìš©ì–´ + LAB ì¶”ê°€ ë¶ˆìš©ì–´ì—ì„œ í•µì‹¬ ë‹¨ì–´ ì¶”ì¶œ (DV_13, 318~325í–‰ ì°¸ê³ )
    base_stopwords = ['ì„œìš¸', 'ì„œìš¸ì‹œ', 'ë¶€ë™ì‚°', 'ì£¼ìš”', 'ê²°ê³¼', 'ì¡°ì‚¬', 'ëŒ€í‘œ', 'ì‹œì ˆ', 'í™œìš©', 'ìš”ì†Œ', 'ì ìš©', 'ì¤‘ì•™', 'ì „ì£¼', 'í•œêµ­', 'í¬í•¨', 'ë„ì‹œ', 'ì¼ë¶€', 'ì´ìŠˆ', 'ë³´ê³ ì„œ', 'ê°ˆë“±', 'ë¯¸ë˜', 'ìœ„ì›', 'í†µí•´', 'ë¬¸ì œ']
    stopwords = set(base_stopwords)
    stopwords.update(stopwords_add) 
    
    all_nouns = []
    text_data = df['title'] + ' ' + df['description']

    for text in text_data:
        # ì •ì œ: í•œê¸€, ì˜ì–´, ìˆ«ì ì™¸ ì œê±°
        text_cleaned = re.sub(r'[^ê°€-í£A-Za-z0-9\s]', ' ', text)
        
        # ëª…ì‚¬ ì¶”ì¶œ
        nouns = okt.nouns(text_cleaned)
        
        # ë¶ˆìš©ì–´ ë° ê¸¸ì´ í•„í„°ë§ (DV_13, 346í–‰ ì°¸ê³ )
        filtered_nouns = [word for word in set(nouns) if (len(word) >= min_len) and (word not in stopwords)]
        all_nouns.append(filtered_nouns)

    # ------------------------------------------------
    # 3.1. WordCloudìš© ì „ì²´ ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸
    # ------------------------------------------------
    total_nouns = sum(all_nouns, [])

    # ------------------------------------------------
    # 3.2. NetworkXìš© ì—£ì§€ ë° ê°€ì¤‘ì¹˜ ê³„ì‚° (DV_14, 363~379í–‰ ì°¸ê³ )
    # ------------------------------------------------
    edge_list = []
    for nouns in all_nouns:
        if len(nouns) > 1:
            edge_list.extend(combinations(sorted(nouns), 2))
    
    edge_counts = Counter(edge_list)
    
    # ìµœì†Œ ë¹ˆë„ ì´ìƒ ì—£ì§€ í•„í„°ë§
    filtered_edges = {edge: weight for edge, weight in edge_counts.items() if weight >= min_count}
    
    # NetworkX ê·¸ë˜í”„ ìƒì„± ë° ì—£ì§€ ì¶”ê°€
    G = nx.Graph()
    weighted_edges = [(node1, node2, {'weight': weight}) 
                      for (node1, node2), weight in filtered_edges.items()]
    G.add_edges_from(weighted_edges)

    return total_nouns, filtered_edges, G
# %%
#streamlit ë©”ì¸ ëŒ€ì‹œë³´ë“œ êµ¬í˜„
def main():
    st.title("ğŸ¤ K-POP ë°ëª¬ í—Œí„°ìŠ¤ íŒ¬ë¤ ë¶„ì„ ëŒ€ì‹œë³´ë“œ")
    st.markdown("---")
    
    # ------------------------------------------------
    # 4.1. Sidebar: ìœ„ì ¯ì„ í™œìš©í•œ ì¸í„°ë™í‹°ë¸Œ ì„¤ì • (5ê°œ ì´ìƒ ìœ„ì ¯ ì¶©ì¡±)
    # ------------------------------------------------
    
    # 1. ê²€ìƒ‰ì–´ ì…ë ¥ (text_input)
    search_query = st.sidebar.text_input("1. ë¶„ì„í•  K-POP ê·¸ë£¹ëª…/í‚¤ì›Œë“œ:", "K-POP ë°ëª¬ í—Œí„°ìŠ¤")
    
    # 2. ë°ì´í„° ìˆ˜ì§‘ ê°œìˆ˜ (slider)
    num_data = st.sidebar.slider("2. ìˆ˜ì§‘í•  ë°ì´í„°(ë‰´ìŠ¤) ê°œìˆ˜:", 100, 1000, 500, step=100) # DV_11, 2095í–‰ ì°¸ì¡°
    
    # 3. ìµœì†Œ ë‹¨ì–´ ê¸¸ì´ (number_input)
    min_len = st.sidebar.number_input("3. ìµœì†Œ ë‹¨ì–´ ê¸¸ì´ (Min Length):", 2, 5, 2)
    
    # 4. ë„¤íŠ¸ì›Œí¬ ìµœì†Œ ì—°ê²° ë¹ˆë„ (slider)
    min_count = st.sidebar.slider("4. ë„¤íŠ¸ì›Œí¬ ìµœì†Œ ì—°ê²° ë¹ˆë„ (Min Count):", 1, 30, 10)
    
    # 5. ì‚¬ìš©ì ì¶”ê°€ ë¶ˆìš©ì–´ (text_area)
    stopwords_input = st.sidebar.text_area("5. ì¶”ê°€í•  ë¶ˆìš©ì–´ (ì‰¼í‘œë¡œ êµ¬ë¶„):", "ë©¤ë²„, ê°€ìˆ˜, ê·¸ë£¹, ëª…, ì•¨ë²”, ì»´ë°±, ë¬´ëŒ€, ì›”, ì¼")
    stopwords_add = [s.strip() for s in stopwords_input.split(',') if s.strip()]

    # 6. ì‹¤í–‰ ë²„íŠ¼ (button, 6ë²ˆì§¸ ìœ„ì ¯)
    run_analysis = st.sidebar.button("ğŸ“Š ë¶„ì„ ì‹¤í–‰ ë° ëŒ€ì‹œë³´ë“œ ì—…ë°ì´íŠ¸")

    # ë„¤ì´ë²„ API í‚¤ (ì‹œí—˜ ë¬¸ì œì—ì„œ ì œê³µëœ ê°’)
    client_id = 'Hl5maeWyGFS0SOj9hJQt'
    client_secret = 'sYYE75Wqpv'

#%%
#4.2 ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬
if run_analysis or 'data' not in st.session_state:
        # ë°ì´í„° ìˆ˜ì§‘ (get_naver_news_data í•¨ìˆ˜ í˜¸ì¶œ)
        df_raw = get_naver_news_data(search_query, num_data, client_id, client_secret)
        
        # ì „ì²˜ë¦¬ ë° ë¶„ì„ (preprocess_and_analyze í•¨ìˆ˜ í˜¸ì¶œ)
        total_nouns, filtered_edges, G = preprocess_and_analyze(df_raw, min_count, min_len, stopwords_add)
        
        # Session Stateì— ê²°ê³¼ ì €ì¥ (DV_12 ì„¸ì…˜ ìƒíƒœ ì°¸ì¡°)
        st.session_state['data'] = df_raw
        st.session_state['nouns'] = total_nouns
        st.session_state['edges'] = filtered_edges
        st.session_state['graph'] = G
        st.session_state['query'] = search_query

    # Session Stateì—ì„œ ê²°ê³¼ ë¶ˆëŸ¬ì˜¤ê¸°
    df = st.session_state.get('data', pd.DataFrame())
    total_nouns = st.session_state.get('nouns', [])
    G = st.session_state.get('graph', nx.Graph())
    search_query = st.session_state.get('query', "K-POP ë°ëª¬ í—Œí„°ìŠ¤")
# %%
# ë©”íŠ¸ë¦­ 
st.subheader(f"ğŸ” '{search_query}'ì— ëŒ€í•œ ë°ì´í„° í˜„í™©")
    col_metric1, col_metric2, col_metric3, col_metric4 = st.columns(4)

    col_metric1.metric("ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜", f"{len(df)}ê±´")
    col_metric2.metric("ë¶„ì„ëœ ì´ ë‹¨ì–´ ìˆ˜", f"{len(total_nouns)}ê°œ")
    col_metric3.metric("ë„¤íŠ¸ì›Œí¬ ë…¸ë“œ(í‚¤ì›Œë“œ) ìˆ˜", f"{G.number_of_nodes()}ê°œ")
    col_metric4.metric("ë„¤íŠ¸ì›Œí¬ ì—£ì§€(ì—°ê²°) ìˆ˜", f"{G.number_of_edges()}ê°œ")

    if not df.empty:
        with st.expander("ì›ì²œ ë°ì´í„°(ë‰´ìŠ¤ ê¸°ì‚¬) ë¯¸ë¦¬ë³´ê¸°"): # í™•ì¥ ë ˆì´ì•„ì›ƒ (DV_12 ì°¸ì¡°)
            st.dataframe(df[['pubDate', 'title', 'description']].head(10), use_container_width=True)

    st.markdown("---")
# %%
# ì‹œê°í™” 
tab1, tab2, tab3 = st.tabs(["ğŸ“Š ì‹œê³„ì—´ ë° ë¹ˆë„ ë¶„ì„", "â˜ï¸ í•µì‹¬ í‚¤ì›Œë“œ WordCloud", "ğŸ•¸ï¸ í‚¤ì›Œë“œ ê´€ê³„ë§ ë„¤íŠ¸ì›Œí¬"])
    
    with tab1: # Plotly (ì‹œê³„ì—´) ë° Seaborn (ë¹ˆë„)
        st.header("1. ì‹œê³„ì—´ ë° ë¹ˆë„ ê¸°ë°˜ ë¶„ì„: íŒ¬ë¤ í˜•ì„± ìš”ì¸ ì¶”ì´")
        st.markdown("ë‰´ìŠ¤ ê¸°ì‚¬ ë°œí–‰ ì‹œì ì˜ íŠ¸ë Œë“œ ë³€í™”ì™€ í‚¤ì›Œë“œ ë¹ˆë„ë¥¼ í†µí•´ íŒ¬ë¤ì˜ ì£¼ìš” ê´€ì‹¬ì‚¬ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.")
        
        col_plot1, col_plot2 = st.columns(2)
        
        with col_plot1: # Plotly ê·¸ë˜í”„ (ìš”êµ¬ì‚¬í•­ 1 ì¶©ì¡±)
            st.subheader("ë°œí–‰ì¼ìë³„ ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ ì¶”ì´ (Plotly)")
            if not df.empty:
                df_counts = df.groupby(df['pubDate'].dt.date).size().reset_index(name='count')
                df_counts['date'] = pd.to_datetime(df_counts['pubDate'])
                
                # Plotly Expressë¥¼ ì´ìš©í•œ ì‹œê³„ì—´ ë¼ì¸ ì°¨íŠ¸
                fig_plotly = px.line(df_counts, x='date', y='count', 
                                     title='ì‹œê°„ ê²½ê³¼ì— ë”°ë¥¸ ì •ë³´ëŸ‰ ë³€í™”')
                st.plotly_chart(fig_plotly, use_container_width=True)
            else:
                st.warning("ë°ì´í„°ê°€ ì—†ì–´ ì‹œê³„ì—´ ë¶„ì„ì„ í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        with col_plot2: # Seaborn/Matplotlib ê·¸ë˜í”„ (ìš”êµ¬ì‚¬í•­ 3 ì¶©ì¡±)
            st.subheader("ìƒìœ„ 15ê°œ í‚¤ì›Œë“œ ë¹ˆë„ (Seaborn)")
            if total_nouns:
                word_counts = Counter(total_nouns).most_common(15)
                df_word_counts = pd.DataFrame(word_counts, columns=['Keyword', 'Frequency'])
                
                # Matplotlib Figure ìƒì„±
                fig_sns, ax_sns = plt.subplots(figsize=(10, 6))
                sns.barplot(x='Frequency', y='Keyword', data=df_word_counts, ax=ax_sns, palette='viridis')
                ax_sns.set_title('í‚¤ì›Œë“œ ë¹ˆë„ Top 15')
                ax_sns.set_xlabel('ë¹ˆë„ìˆ˜')
                ax_sns.set_ylabel('í‚¤ì›Œë“œ')
                plt.tight_layout()
                st.pyplot(fig_sns)
            else:
                st.warning("ë¶„ì„í•  ë‹¨ì–´ê°€ ì—†ì–´ ë¹ˆë„ ë¶„ì„ì„ í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    with tab2: # WordCloud (WordCloud ìš”êµ¬ì‚¬í•­ ì¶©ì¡±)
        st.header("2. í•µì‹¬ í‚¤ì›Œë“œ WordCloud: íŒ¬ë¤ì˜ í•µì‹¬ ê´€ì‹¬ì‚¬")
        st.markdown("í¬ê¸°ê°€ í´ìˆ˜ë¡ ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ ìì£¼ ì–¸ê¸‰ë˜ëŠ” ë‹¨ì–´ë¡œ, íŒ¬ë¤ì´ í˜•ì„±ë˜ëŠ” **í•µì‹¬ ìš”ì¸**ì„ ì§ê´€ì ìœ¼ë¡œ íŒŒì•…í•©ë‹ˆë‹¤.")
        
        if total_nouns:
            words_text = " ".join(total_nouns)
            
            # WordCloud ê°ì²´ ìƒì„± (DV_13, 1575í–‰ ì´í›„ ì°¸ì¡°)
            wordcloud = WordCloud(
                font_path=plt.rcParams['font.family'][0], # ì„¤ì •ëœ í°íŠ¸ ì‚¬ìš©
                max_words=100,
                width=1000, 
                height=600,
                background_color='black',
                colormap='coolwarm',
                stopwords=STOPWORDS
            ).generate(words_text)

            fig_wc, ax_wc = plt.subplots(figsize=(10, 6))
            ax_wc.imshow(wordcloud, interpolation='bilinear')
            ax_wc.axis('off')
            st.pyplot(fig_wc)
        else:
            st.warning("ë¶„ì„í•  ë‹¨ì–´ê°€ ì—†ì–´ WordCloudë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
    with tab3: # NetworkX (ë„¤íŠ¸ì›Œí¬ ì‹œê°í™” ìš”êµ¬ì‚¬í•­ ì¶©ì¡±)
        st.header("3. í‚¤ì›Œë“œ ê´€ê³„ë§ ë¶„ì„: íŒ¬ë¤ ë‚´ **ì—°ê²° êµ¬ì¡°**")
        st.markdown(f"ë…¸ë“œëŠ” í‚¤ì›Œë“œ, ì—£ì§€ëŠ” ë™ì‹œ ë“±ì¥ ë¹ˆë„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì—°ê²°ì´ ê°•í• ìˆ˜ë¡(êµµì€ ì„ ) í‚¤ì›Œë“œ ê°„ ì—°ê´€ì„±ì´ ë†’ìŠµë‹ˆë‹¤. (ìµœì†Œ ì—°ê²° ë¹ˆë„: {min_count})")
        
        if G.number_of_nodes() > 0:
            # NetworkX ì‹œê°í™” (DV_14, 411~455í–‰ ì°¸ì¡°)
            pos_spring = nx.spring_layout(G, k=0.3, iterations=50, seed=42)
            node_sizes = [G.degree(node) * 500 / G.number_of_nodes() for node in G.nodes()]
            edge_widths = [G[u][v]['weight'] * 0.05 for u, v in G.edges()]
            
            fig_net, ax_net = plt.subplots(figsize=(15, 15))

            nx.draw_networkx(
                G, pos_spring, 
                with_labels=True, 
                node_size=node_sizes, 
                width=edge_widths,
                font_size=10, 
                node_color='lightcoral', 
                edge_color='gray', 
                alpha=0.7,
                ax=ax_net
            )
            ax_net.axis('off')
            ax_net.set_title("í‚¤ì›Œë“œ ê´€ê³„ë§ (NetworkX)", size=18)
            st.pyplot(fig_net)
        else:
            st.warning(f"ì„¤ì •ëœ ìµœì†Œ ì—°ê²° ë¹ˆë„({min_count}) ê¸°ì¤€ìœ¼ë¡œ ìƒì„±ëœ ë„¤íŠ¸ì›Œí¬ê°€ ì—†ìŠµë‹ˆë‹¤. ì˜µì…˜ì„ ì¡°ì •í•´ ë³´ì„¸ìš”.")
            
    st.markdown("---")
# %%
#ê²°ë¡ 
    st.header("ğŸ“ ì¢…í•© ê²°ë¡  ë° ë¶„ì„ í•´ì„")
    st.success("ë°ì´í„° ì‹œê°í™” ê²°ê³¼ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
    st.markdown("""
        **1. ê¸°íš ì˜ë„: ë‹¤ê°ì  íŒ¬ë¤ í˜•ì„± ìš”ì¸ ë¶„ì„**
        ë³¸ ëŒ€ì‹œë³´ë“œëŠ” K-POP ë°ëª¬ í—Œí„°ìŠ¤ì— ëŒ€í•œ ì˜¨ë¼ì¸ ì—¬ë¡ ì„ ì‹œê³„ì—´, ë¹ˆë„, ê´€ê³„ë§ì˜ ì„¸ ê°€ì§€ ì‹œê°ìœ¼ë¡œ ë¶„ì„í•˜ì—¬, íŒ¬ë¤ í˜•ì„±ì˜ í•µì‹¬ ìš”ì¸(í™œë™, ì½˜í…ì¸ , ë©¤ë²„ ë“±)ì„ íŒŒì•…í•˜ëŠ” ë° ì¤‘ì ì„ ë‘¡ë‹ˆë‹¤.
        
        **2. ì£¼ìš” ì‹œê°í™” ê²°ê³¼ í•´ì„**
        * **ì‹œê³„ì—´ ë¶„ì„ (Plotly):** ë‰´ìŠ¤ ê¸°ì‚¬ ë°œí–‰ ì¶”ì´ì—ì„œ íŠ¹ì • ì‹œì  (ì˜ˆ: ì‹ ê·œ ì•¨ë²” ë°œë§¤, ì£¼ìš” ìˆ˜ìƒ)ì— ì •ë³´ëŸ‰ì´ í­ì¦í•˜ëŠ” íŒ¨í„´ì´ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” íŒ¬ë¤ì´ íŠ¹ì • 'ì´ë²¤íŠ¸'ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ê²°ì§‘í•˜ëŠ” ê²½í–¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
        * **ë¹ˆë„/WordCloud ë¶„ì„:** 'ì½˜í…ì¸ ', 'ì„±ì¥', 'ìŠ¤í† ë¦¬', 'ì„¸ê³„ê´€' ë“±ê³¼ ê°™ì€ ë‹¨ì–´ê°€ ë†’ì€ ë¹ˆë„ë¥¼ ë³´ì¸ë‹¤ë©´, íŒ¬ë¤ì´ ë‹¨ìˆœí•œ ìŒì•…ì  ìš”ì†Œ ì™¸ì— ê·¸ë£¹ì˜ **ì„œì‚¬(Narrative)ì™€ ë©”ì‹œì§€**ì— ê¹Šì´ ê´€ì—¬í•˜ê³  ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.
        * **ê´€ê³„ë§ ë¶„ì„ (NetworkX):** ë§Œì•½ 'OOOë©¤ë²„'ì™€ 'ê°œì¸í™œë™'ì´ ê°•í•˜ê²Œ ì—°ê²°ë˜ê³ , ì´ ì—°ê²°ì´ 'í•´ì™¸ë°˜ì‘'ê³¼ë„ êµµì€ ì„ ìœ¼ë¡œ ì´ì–´ì§„ë‹¤ë©´, íŠ¹ì • ë©¤ë²„ì˜ ê°œë³„ í™œë™ì´ íŒ¬ë¤ì˜ ì™¸ì—° í™•ì¥ê³¼ ê·¸ë£¹ì˜ ê¸€ë¡œë²Œ ì¸ì§€ë„ ìƒìŠ¹ì— **ê²°ì •ì ì¸ ì¤‘ê°œì ì—­í• (ë§¤ê°œ ì¤‘ì‹¬ì„±)**ì„ í–ˆìŒì„ ë…¼ë¦¬ì ìœ¼ë¡œ ì¶”ë¡ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """)
    

if __name__ == "__main__":
    # í•œê¸€ í°íŠ¸ê°€ ì—†ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ê²½ê³ 
    if plt.rcParams['font.family'][0] == 'DejaVu Sans':
        st.warning("âš ï¸ Streamlit Cloud í™˜ê²½ì—ì„œ í•œê¸€ í°íŠ¸ê°€ ê¹¨ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Streamlit Cloud secretsë¥¼ í†µí•´ í°íŠ¸ë¥¼ ì—…ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤.")
        
    main()
# %%
